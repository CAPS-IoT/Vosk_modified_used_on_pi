{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdqvrs5TYr2Y",
        "outputId": "206d38b4-0819-488a-eb05-53a80d766bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/xuwei/training\n",
            "--2025-04-11 17:00:55--  https://alphacephei.com/vosk-colab/kaldi.tar.gz\n",
            "正在解析主机 alphacephei.com (alphacephei.com)... 2a01:4f8:13a:279f::2, 188.40.21.16\n",
            "正在连接 alphacephei.com (alphacephei.com)|2a01:4f8:13a:279f::2|:443... 已连接。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xuwei/vosk_build2/venv/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已发出 HTTP 请求，正在等待回应... 200 OK\n",
            "长度： 1041599901 (993M) [application/octet-stream]\n",
            "正在保存至: ‘kaldi.tar.gz’\n",
            "\n",
            "kaldi.tar.gz        100%[===================>] 993.35M  13.1MB/s    用时 76s     \n",
            "\n",
            "2025-04-11 17:02:11 (13.0 MB/s) - 已保存 ‘kaldi.tar.gz’ [1041599901/1041599901])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /home/xuwei/training\n",
        "!wget -c https://alphacephei.com/vosk-colab/kaldi.tar.gz\n",
        "!tar xzf kaldi.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The standard file /tools/config/common_path.sh is not present -> Exit!\n"
          ]
        }
      ],
      "source": [
        "!export KALDI_ROOT=/home/xuwei/training/kaldi\n",
        "\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/xuwei/training/kaldi/tools/openfst-1.8.0/lib/fst\n",
        "!source /home/xuwei/training/vosk-api/training/cmd.sh\n",
        "!source /home/xuwei/training/vosk-api/training/path.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfUtX42VZ1zl",
        "outputId": "0cbed826-bac2-4e15-dfa2-2a101b3562dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/xuwei/training\n",
            "正克隆到 'vosk-api'...\n",
            "remote: Enumerating objects: 4380, done.\u001b[K\n",
            "remote: Counting objects: 100% (700/700), done.\u001b[K\n",
            "remote: Compressing objects: 100% (250/250), done.\u001b[K\n",
            "remote: Total 4380 (delta 592), reused 450 (delta 450), pack-reused 3680 (from 2)\u001b[K\n",
            "接收对象中: 100% (4380/4380), 14.15 MiB | 12.46 MiB/s, 完成.\n",
            "处理 delta 中: 100% (2421/2421), 完成.\n"
          ]
        }
      ],
      "source": [
        "%cd /home/xuwei/training\n",
        "!rm -rf vosk-api\n",
        "!git clone https://github.com/alphacep/vosk-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bwdTkMVpZ4Pb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shell-init: 获取当前目录时出错：getcwd: 无法访问父目录：没有那个文件或目录\n",
            "shell-init: 获取当前目录时出错：getcwd: 无法访问父目录：没有那个文件或目录\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /home/xuwei/training/kaldi/egs/ac/training\n",
        "!cp -r /home/xuwei/training/vosk-api/training /home/xuwei/training/kaldi/egs/ac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvC_AENtasCF",
        "outputId": "d0740818-836c-40d4-9e44-3f966e83128c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mE: \u001b[0m无法打开锁文件 /var/lib/dpkg/lock-frontend - open (13: 权限不够)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0m无法获取 dpkg 前端锁 (/var/lib/dpkg/lock-frontend)，请查看您是否正以 root 用户运行？\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!apt install flac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv1cbjZuaE6O",
        "outputId": "13cb7312-97d6-42ec-910a-8095ba1eb967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/xuwei/training/kaldi/egs/ac/training\n",
            "#!/usr/bin/env bash\n",
            "\n",
            ". ./cmd.sh\n",
            ". ./path.sh\n",
            "\n",
            "stage=-1\n",
            "stop_stage=100\n",
            "\n",
            ". utils/parse_options.sh\n",
            "\n",
            "# Data preparation\n",
            "if [ ${stage} -le 0 ] && [ ${stop_stage} -ge 0 ]; then\n",
            "  data_url=www.openslr.org/resources/31\n",
            "  lm_url=www.openslr.org/resources/11\n",
            "  database=corpus\n",
            "\n",
            "  mkdir -p $database\n",
            "  for part in dev-clean-2 train-clean-5; do\n",
            "    local/download_and_untar.sh $database $data_url $part\n",
            "  done\n",
            "\n",
            "  local/download_lm.sh $lm_url $database data/local/lm\n",
            "\n",
            "  local/data_prep.sh $database/LibriSpeech/train-clean-5 data/train\n",
            "  local/data_prep.sh $database/LibriSpeech/dev-clean-2 data/test\n",
            "fi\n",
            "\n",
            "# Dictionary formatting\n",
            "if [ ${stage} -le 1 ] && [ ${stop_stage} -ge 1 ]; then\n",
            "  local/prepare_dict.sh data/local/lm data/local/dict\n",
            "  utils/prepare_lang.sh data/local/dict \"<UNK>\" data/local/lang data/lang\n",
            "fi\n",
            "\n",
            "# Extract MFCC features\n",
            "if [ ${stage} -le 2 ] && [ ${stop_stage} -ge 2 ]; then\n",
            "  for task in train; do\n",
            "    steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 2 data/$task exp/make_mfcc/$task $mfcc\n",
            "    steps/compute_cmvn_stats.sh data/$task exp/make_mfcc/$task $mfcc\n",
            "  done\n",
            "fi\n",
            "\n",
            "# Train GMM models\n",
            "if [ ${stage} -le 3 ] && [ ${stop_stage} -ge 3 ]; then\n",
            "  steps/train_mono.sh --nj 2 --cmd \"$train_cmd\" \\\n",
            "    data/train data/lang exp/mono\n",
            "\n",
            "  steps/align_si.sh  --nj 2 --cmd \"$train_cmd\" \\\n",
            "    data/train data/lang exp/mono exp/mono_ali\n",
            "\n",
            "  steps/train_lda_mllt.sh  --cmd \"$train_cmd\" \\\n",
            "    2000 10000 data/train data/lang exp/mono_ali exp/tri1\n",
            "\n",
            "  steps/align_si.sh --nj 2 --cmd \"$train_cmd\" \\\n",
            "    data/train data/lang exp/tri1 exp/tri1_ali\n",
            "\n",
            "  steps/train_lda_mllt.sh --cmd \"$train_cmd\" \\\n",
            "    2500 15000 data/train data/lang exp/tri1_ali exp/tri2\n",
            "\n",
            "  steps/align_si.sh  --nj 2 --cmd \"$train_cmd\" \\\n",
            "    data/train data/lang exp/tri2 exp/tri2_ali\n",
            "\n",
            "  steps/train_lda_mllt.sh --cmd \"$train_cmd\" \\\n",
            "    2500 20000 data/train data/lang exp/tri2_ali exp/tri3\n",
            "\n",
            "  steps/align_si.sh  --nj 2 --cmd \"$train_cmd\" \\\n",
            "    data/train data/lang exp/tri3 exp/tri3_ali\n",
            "fi\n",
            "\n",
            "# Train TDNN model\n",
            "if [ ${stage} -le 4 ] && [ ${stop_stage} -ge 4 ]; then\n",
            "  local/chain/run_tdnn.sh\n",
            "fi\n",
            "\n",
            "# Decode\n",
            "if [ ${stage} -le 5 ] && [ ${stop_stage} -ge 5 ]; then\n",
            "\n",
            "  utils/format_lm.sh data/lang data/local/lm/lm_tgsmall.arpa.gz data/local/dict/lexicon.txt data/lang_test\n",
            "  utils/mkgraph.sh --self-loop-scale 1.0 data/lang_test exp/chain/tdnn exp/chain/tdnn/graph\n",
            "  utils/build_const_arpa_lm.sh data/local/lm/lm_tgmed.arpa.gz \\\n",
            "    data/lang data/lang_test_rescore\n",
            "\n",
            "  for task in test; do\n",
            "\n",
            "    steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 2 data/$task exp/make_mfcc/$task $mfcc\n",
            "    steps/compute_cmvn_stats.sh data/$task exp/make_mfcc/$task $mfcc\n",
            "\n",
            "    steps/online/nnet2/extract_ivectors_online.sh --nj 2 \\\n",
            "        data/${task} exp/chain/extractor \\\n",
            "        exp/chain/ivectors_${task}\n",
            "\n",
            "    steps/nnet3/decode.sh --cmd $decode_cmd --num-threads 10 --nj 1 \\\n",
            "         --beam 13.0 --max-active 7000 --lattice-beam 4.0 \\\n",
            "         --online-ivector-dir exp/chain/ivectors_${task} \\\n",
            "         --acwt 1.0 --post-decode-acwt 10.0 \\\n",
            "         exp/chain/tdnn/graph data/${task} exp/chain/tdnn/decode_${task}\n",
            "\n",
            "    steps/lmrescore_const_arpa.sh data/lang_test data/lang_test_rescore \\\n",
            "        data/${task} exp/chain/tdnn/decode_${task} exp/chain/tdnn/decode_${task}_rescore\n",
            "  done\n",
            "\n",
            "  bash RESULTS\n",
            "fi\n",
            "local/download_and_untar.sh: data part dev-clean-2 was already successfully extracted, nothing to do.\n",
            "local/download_and_untar.sh: data part train-clean-5 was already successfully extracted, nothing to do.\n",
            "Downloading file '3-gram.pruned.1e-7.arpa.gz' into 'corpus'...\n",
            "'3-gram.pruned.1e-7.arpa.gz' already exists and appears to be complete\n",
            "Downloading file '3-gram.pruned.3e-7.arpa.gz' into 'corpus'...\n",
            "'3-gram.pruned.3e-7.arpa.gz' already exists and appears to be complete\n",
            "Downloading file 'librispeech-vocab.txt' into 'corpus'...\n",
            "'librispeech-vocab.txt' already exists and appears to be complete\n",
            "Downloading file 'librispeech-lexicon.txt' into 'corpus'...\n",
            "'librispeech-lexicon.txt' already exists and appears to be complete\n",
            "utils/validate_data_dir.sh: Successfully validated data-directory data/train\n",
            "local/data_prep.sh: successfully prepared data in data/train\n",
            "utils/validate_data_dir.sh: Successfully validated data-directory data/test\n",
            "local/data_prep.sh: successfully prepared data in data/test\n",
            "Preparing phone lists and lexicon\n",
            "utils/prepare_lang.sh data/local/dict <UNK> data/local/lang data/lang\n",
            "Checking data/local/dict/silence_phones.txt ...\n",
            "--> reading data/local/dict/silence_phones.txt\n",
            "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
            "--> text contains only allowed whitespaces\n",
            "--> data/local/dict/silence_phones.txt is OK\n",
            "\n",
            "Checking data/local/dict/optional_silence.txt ...\n",
            "--> reading data/local/dict/optional_silence.txt\n",
            "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
            "--> text contains only allowed whitespaces\n",
            "--> data/local/dict/optional_silence.txt is OK\n",
            "\n",
            "Checking data/local/dict/nonsilence_phones.txt ...\n",
            "--> reading data/local/dict/nonsilence_phones.txt\n",
            "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
            "--> text contains only allowed whitespaces\n",
            "--> data/local/dict/nonsilence_phones.txt is OK\n",
            "\n",
            "Checking disjoint: silence_phones.txt, nonsilence_phones.txt\n",
            "--> disjoint property is OK.\n",
            "\n",
            "Checking data/local/dict/lexicon.txt\n",
            "--> reading data/local/dict/lexicon.txt\n",
            "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
            "--> text contains only allowed whitespaces\n",
            "--> data/local/dict/lexicon.txt is OK\n",
            "\n",
            "Checking data/local/dict/lexiconp.txt\n",
            "--> reading data/local/dict/lexiconp.txt\n",
            "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
            "--> text contains only allowed whitespaces\n",
            "--> data/local/dict/lexiconp.txt is OK\n",
            "\n",
            "Checking lexicon pair data/local/dict/lexicon.txt and data/local/dict/lexiconp.txt\n",
            "--> lexicon pair data/local/dict/lexicon.txt and data/local/dict/lexiconp.txt match\n",
            "\n",
            "Checking data/local/dict/extra_questions.txt ...\n",
            "--> data/local/dict/extra_questions.txt is empty (this is OK)\n",
            "--> SUCCESS [validating dictionary directory data/local/dict]\n",
            "\n",
            "fstcompile: error while loading shared libraries: libfstscript.so.23: cannot open shared object file: No such file or directory\n",
            "fstarcsort: error while loading shared libraries: libfstscript.so.23: cannot open shared object file: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/xuwei/training/kaldi/egs/ac/training/utils/lang/make_lexicon_fst.py\", line 411, in <module>\n",
            "    main()\n",
            "  File \"/home/xuwei/training/kaldi/egs/ac/training/utils/lang/make_lexicon_fst.py\", line 398, in main\n",
            "    write_fst_with_silence(lexicon, args.sil_prob, args.sil_phone,\n",
            "  File \"/home/xuwei/training/kaldi/egs/ac/training/utils/lang/make_lexicon_fst.py\", line 274, in write_fst_with_silence\n",
            "    print(\"{src}\\t{dest}\\t{phone}\\t{word}\\t{cost}\".format(\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "steps/make_mfcc.sh --cmd run.pl --nj 2 data/train exp/make_mfcc/train\n",
            "utils/validate_data_dir.sh: Successfully validated data-directory data/train\n",
            "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
            "run.pl: 2 / 2 failed, log is in exp/make_mfcc/train/make_mfcc_train.*.log\n",
            "steps/compute_cmvn_stats.sh data/train exp/make_mfcc/train\n",
            "steps/compute_cmvn_stats.sh: no such file data/train/feats.scp\n",
            "steps/train_mono.sh --nj 2 --cmd run.pl data/train data/lang exp/mono\n",
            "cat: data/lang/oov.int: No such file or directory\n",
            "steps/align_si.sh --nj 2 --cmd run.pl data/train data/lang exp/mono exp/mono_ali\n",
            "steps/align_si.sh: expected file data/lang/oov.int to exist\n",
            "steps/train_lda_mllt.sh --cmd run.pl 2000 10000 data/train data/lang exp/mono_ali exp/tri1\n",
            "train_lda_mllt.sh: no such file exp/mono_ali/final.mdl\n",
            "steps/align_si.sh --nj 2 --cmd run.pl data/train data/lang exp/tri1 exp/tri1_ali\n",
            "steps/align_si.sh: expected file data/lang/oov.int to exist\n",
            "steps/train_lda_mllt.sh --cmd run.pl 2500 15000 data/train data/lang exp/tri1_ali exp/tri2\n",
            "train_lda_mllt.sh: no such file exp/tri1_ali/final.mdl\n",
            "steps/align_si.sh --nj 2 --cmd run.pl data/train data/lang exp/tri2 exp/tri2_ali\n",
            "steps/align_si.sh: expected file data/lang/oov.int to exist\n",
            "steps/train_lda_mllt.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri2_ali exp/tri3\n",
            "train_lda_mllt.sh: no such file exp/tri2_ali/final.mdl\n",
            "steps/align_si.sh --nj 2 --cmd run.pl data/train data/lang exp/tri3 exp/tri3_ali\n",
            "steps/align_si.sh: expected file data/lang/oov.int to exist\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /home/xuwei/training/kaldi/egs/ac/training\n",
        "!rm -rf exp\n",
        "!sed -i 's:--nj 10:--nj 2:g' run.sh\n",
        "!cat run.sh\n",
        "!bash run.sh --stop-stage 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWKGX5f-fpKJ",
        "outputId": "26dcf7dc-a08b-46bb-d72d-6c52397c287d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#!/bin/bash\n",
            "\n",
            "# Set -e here so that we catch if any executable fails immediately\n",
            "set -euo pipefail\n",
            "\n",
            "# (some of which are also used in this script directly).\n",
            "stage=-1\n",
            "decode_nj=10\n",
            "train_set=train\n",
            "gmm=tri3\n",
            "nnet3_affix=\n",
            "suffix=\n",
            "\n",
            "# The rest are configs specific to this script.  Most of the parameters\n",
            "# are just hardcoded at this level, in the commands below.\n",
            "affix=   # affix for the TDNN directory name\n",
            "tree_affix=\n",
            "train_stage=-10\n",
            "get_egs_stage=-10\n",
            "decode_iter=\n",
            "\n",
            "# training options\n",
            "# training chunk-options\n",
            "chunk_width=140,100,160\n",
            "common_egs_dir=\n",
            "xent_regularize=0.1\n",
            "dropout_schedule='0,0@0.20,0.5@0.50,0'\n",
            "\n",
            "# training options\n",
            "srand=0\n",
            "remove_egs=true\n",
            "\n",
            "# End configuration section.\n",
            "echo \"$0 $@\"  # Print the command line for logging\n",
            "\n",
            ". ./cmd.sh\n",
            ". ./path.sh\n",
            ". ./utils/parse_options.sh\n",
            "\n",
            "# Problem: We have removed the \"train_\" prefix of our training set in\n",
            "# the alignment directory names! Bad!\n",
            "gmm_dir=exp/$gmm\n",
            "ali_dir=exp/${gmm}_ali\n",
            "tree_dir=exp/chain${suffix}/tree${tree_affix:+_$tree_affix}\n",
            "lang=data/lang_chain${suffix}\n",
            "lat_dir=exp/chain${suffix}/${gmm}_${train_set}_lats\n",
            "dir=exp/chain${suffix}/tdnn${affix}\n",
            "train_data_dir=data/${train_set}\n",
            "\n",
            "for f in $gmm_dir/final.mdl $train_data_dir/feats.scp $ali_dir/ali.1.gz; do\n",
            "  [ ! -f $f ] && echo \"$0: expected file $f to exist\" && exit 1\n",
            "done\n",
            "\n",
            "if [ $stage -le 9 ]; then\n",
            "    local/chain/run_ivector_common.sh \\\n",
            "                          --train-set ${train_set} \\\n",
            "                          --gmm ${gmm} \\\n",
            "                          --suffix \"${suffix}\"\n",
            "fi\n",
            "\n",
            "if [ $stage -le 10 ]; then\n",
            "  echo \"$0: creating lang directory $lang with chain-type topology\"\n",
            "  rm -rf $lang\n",
            "  cp -r data/lang $lang\n",
            "  silphonelist=$(cat $lang/phones/silence.csl) \n",
            "  nonsilphonelist=$(cat $lang/phones/nonsilence.csl) \n",
            "  steps/nnet3/chain/gen_topo.py $nonsilphonelist $silphonelist >$lang/topo\n",
            "fi\n",
            "\n",
            "if [ $stage -le 11 ]; then\n",
            "    steps/align_fmllr_lats.sh --nj 20 --cmd \"$train_cmd\" ${train_data_dir} \\\n",
            "         data/lang $gmm_dir $lat_dir\n",
            "fi\n",
            "\n",
            "if [ $stage -le 12 ]; then\n",
            "  steps/nnet3/chain/build_tree.sh \\\n",
            "    --frame-subsampling-factor 3 \\\n",
            "    --context-opts \"--context-width=2 --central-position=1\" \\\n",
            "    --cmd \"$train_cmd\" 2500 ${train_data_dir} \\\n",
            "    $lang $ali_dir $tree_dir\n",
            "fi\n",
            "\n",
            "if [ $stage -le 13 ]; then\n",
            "  echo \"$0: creating neural net configs using the xconfig parser\";\n",
            "\n",
            "  num_targets=$(tree-info $tree_dir/tree | grep num-pdfs | awk '{print $2}')\n",
            "  learning_rate_factor=$(echo \"print (0.5/$xent_regularize)\" | python)\n",
            "\n",
            "  affine_opts=\"l2-regularize=0.008 dropout-proportion=0.0 dropout-per-dim=true dropout-per-dim-continuous=true\"\n",
            "  tdnnf_opts=\"l2-regularize=0.008 dropout-proportion=0.0 bypass-scale=0.75\"\n",
            "  linear_opts=\"l2-regularize=0.008 orthonormal-constraint=-1.0\"\n",
            "  prefinal_opts=\"l2-regularize=0.008\"\n",
            "  output_opts=\"l2-regularize=0.002\"\n",
            "\n",
            "  mkdir -p $dir/configs\n",
            "  cat <<EOF > $dir/configs/network.xconfig\n",
            "\n",
            "  input dim=40 name=ivector\n",
            "  input dim=40 name=input\n",
            "\n",
            "  idct-layer name=idct input=input dim=40 cepstral-lifter=22 affine-transform-file=$dir/configs/idct.mat\n",
            "  batchnorm-component name=batchnorm0 input=idct\n",
            "  spec-augment-layer name=spec-augment freq-max-proportion=0.5 time-zeroed-proportion=0.2 time-mask-max-frames=20\n",
            "  delta-layer name=delta input=spec-augment\n",
            "\n",
            "  no-op-component name=input2 input=Append(delta, ReplaceIndex(ivector, t, 0))\n",
            "\n",
            "  # the first splicing is moved before the lda layer, so no splicing here\n",
            "  relu-batchnorm-dropout-layer name=tdnn1 $affine_opts dim=512 input=input2\n",
            "  tdnnf-layer name=tdnnf2 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=1\n",
            "  tdnnf-layer name=tdnnf3 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=1\n",
            "  tdnnf-layer name=tdnnf4 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=1\n",
            "  tdnnf-layer name=tdnnf5 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=0\n",
            "  tdnnf-layer name=tdnnf6 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  tdnnf-layer name=tdnnf7 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  tdnnf-layer name=tdnnf8 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  tdnnf-layer name=tdnnf9 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  tdnnf-layer name=tdnnf10 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  tdnnf-layer name=tdnnf11 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  tdnnf-layer name=tdnnf12 $tdnnf_opts dim=512 bottleneck-dim=96 time-stride=3\n",
            "  linear-component name=prefinal-l dim=192 $linear_opts\n",
            "\n",
            "  ## adding the layers for chain branch\n",
            "  prefinal-layer name=prefinal-chain input=prefinal-l $prefinal_opts small-dim=192 big-dim=512\n",
            "  output-layer name=output include-log-softmax=false dim=$num_targets $output_opts\n",
            "\n",
            "  # adding the layers for xent branch\n",
            "  prefinal-layer name=prefinal-xent input=prefinal-l $prefinal_opts small-dim=192 big-dim=512\n",
            "  output-layer name=output-xent dim=$num_targets learning-rate-factor=$learning_rate_factor $output_opts\n",
            "\n",
            "EOF\n",
            "  steps/nnet3/xconfig_to_configs.py --xconfig-file $dir/configs/network.xconfig --config-dir $dir/configs/\n",
            "fi\n",
            "\n",
            "if [ $stage -le 14 ]; then\n",
            "  steps/nnet3/chain/train.py --use-gpu false --use-gpu false --use-gpu false --stage $train_stage \\\n",
            "    --cmd \"$cuda_cmd\" \\\n",
            "    --feat.online-ivector-dir exp/chain${suffix}/ivectors_${train_set} \\\n",
            "    --feat.cmvn-opts \"--norm-means=false --norm-vars=false\" \\\n",
            "    --chain.xent-regularize $xent_regularize \\\n",
            "    --chain.leaky-hmm-coefficient 0.1 \\\n",
            "    --chain.l2-regularize 0.0 \\\n",
            "    --chain.apply-deriv-weights false \\\n",
            "    --chain.lm-opts=\"--num-extra-lm-states=2000\" \\\n",
            "    --egs.cmd \"$get_egs_cmd\" \\\n",
            "    --egs.dir \"$common_egs_dir\" \\\n",
            "    --egs.stage $get_egs_stage \\\n",
            "    --egs.opts \"--frames-overlap-per-eg 0 --constrained false\" \\\n",
            "    --egs.chunk-width $chunk_width \\\n",
            "    --trainer.dropout-schedule $dropout_schedule \\\n",
            "    --trainer.add-option=\"--optimization.memory-compression-level=2\" \\\n",
            "    --trainer.num-chunk-per-minibatch 64 \\\n",
            "    --trainer.frames-per-iter 2500000 \\\n",
            "    --trainer.num-epochs 20 \\\n",
            "    --trainer.optimization.num-jobs-initial 1 \\\n",
            "    --trainer.optimization.num-jobs-final 1 \\\n",
            "    --trainer.optimization.initial-effective-lrate 0.001 \\\n",
            "    --trainer.optimization.final-effective-lrate 0.0001 \\\n",
            "    --trainer.max-param-change 2.0 \\\n",
            "    --cleanup.remove-egs $remove_egs \\\n",
            "    --feat-dir $train_data_dir \\\n",
            "    --tree-dir $tree_dir \\\n",
            "    --lat-dir $lat_dir \\\n",
            "    --dir $dir  || exit 1;\n",
            "fi\n",
            "local/chain/run_tdnn.sh \n",
            "/home/xuwei/training/kaldi/egs/ac/training/../../../tools/config/common_path.sh: 行 29: LD_LIBRARY_PATH: 未绑定的变量\n"
          ]
        }
      ],
      "source": [
        "!sed -i 's:--nj 10:--nj 2:g' local/chain/run_tdnn.sh\n",
        "!sed -i 's:steps/nnet3/chain/train.py:steps/nnet3/chain/train.py --use-gpu false:g' local/chain/run_tdnn.sh\n",
        "!cat local/chain/run_tdnn.sh\n",
        "!bash local/chain/run_tdnn.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_6cOADJBluSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: 对 'exp/chain/tdnn/13.mdl' 调用 stat 失败: 没有那个文件或目录\n"
          ]
        }
      ],
      "source": [
        "!cp exp/chain/tdnn/13.mdl exp/chain/tdnn/final.mdl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La4qqC-kl9lf",
        "outputId": "3d0cc930-43a2-4ec9-81b9-8436fed8669f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting 'data/local/lm/lm_tgsmall.arpa.gz' to FST\n",
            "cp: cannot stat 'data/lang/topo': No such file or directory\n",
            "mkgraph.sh: expected data/lang_test/L_disambig.fst to exist\n",
            "cat: data/lang/oov.int: No such file or directory\n",
            "utils/build_const_arpa_lm.sh: can't find oov symbol id in data/lang/oov.int\n",
            "steps/make_mfcc.sh --cmd run.pl --nj 2 data/test exp/make_mfcc/test\n",
            "utils/validate_data_dir.sh: Successfully validated data-directory data/test\n",
            "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
            "run.pl: 2 / 2 failed, log is in exp/make_mfcc/test/make_mfcc_test.*.log\n",
            "steps/compute_cmvn_stats.sh data/test exp/make_mfcc/test\n",
            "steps/compute_cmvn_stats.sh: no such file data/test/feats.scp\n",
            "steps/online/nnet2/extract_ivectors_online.sh --nj 2 data/test exp/chain/extractor exp/chain/ivectors_test\n",
            "steps/online/nnet2/extract_ivectors_online.sh: No such file data/test/feats.scp\n",
            "steps/nnet3/decode.sh --cmd run.pl --num-threads 10 --nj 1 --beam 13.0 --max-active 7000 --lattice-beam 4.0 --online-ivector-dir exp/chain/ivectors_test --acwt 1.0 --post-decode-acwt 10.0 exp/chain/tdnn/graph data/test exp/chain/tdnn/decode_test\n",
            "steps/nnet2/check_ivectors_compatible.sh: WARNING: The directories do not contain iVector ID.\n",
            "steps/nnet2/check_ivectors_compatible.sh: WARNING: That means it's you who's reponsible for keeping \n",
            "steps/nnet2/check_ivectors_compatible.sh: WARNING: the directories compatible\n",
            "utils/lang/check_phones_compatible.sh: Error! Both of the two phones-symbol tables are absent.\n",
            "Please check your command\n",
            "steps/lmrescore_const_arpa.sh data/lang_test data/lang_test_rescore data/test exp/chain/tdnn/decode_test exp/chain/tdnn/decode_test_rescore\n",
            "steps/lmrescore_const_arpa.sh: Missing file data/lang_test/G.fst\n"
          ]
        }
      ],
      "source": [
        "!bash run.sh --stage 5"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Vosk Training",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
